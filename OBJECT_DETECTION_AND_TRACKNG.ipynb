{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "CSE5CV Assignment 2"
      ],
      "metadata": {
        "id": "YLKxJTnqWSlp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "STUDENT_ID = 21547932\n"
      ],
      "metadata": {
        "id": "cTXK9ay1WV5P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Importing Required Libraries  \n",
        "\n",
        "In this part, I am bringing in all the external tools (called libraries) that I will use later in my project.  \n",
        "\n",
        "- `copy`, `math`, `numpy`: basic utilities for mathematical calculations and handling arrays of numbers.  \n",
        "- `cv2` (OpenCV): used to open videos, read frames, and process images.  \n",
        "- `matplotlib.pyplot`: used to show images and graphs in the notebook.  \n",
        "- `scipy` and `scipy.optimize`: advanced math tools, sometimes useful for optimization or numerical tasks.  \n",
        "- `torch` and `torchvision`: PyTorch libraries, used for deep learning models and computer vision tasks.  \n",
        "- `torchvision.transforms.functional`: small helper functions to prepare images for models.  \n",
        "- `google.colab.drive`: used to connect my Google Drive so I can read and save files (like my video and results).  \n"
      ],
      "metadata": {
        "id": "V74cN3dqwJAE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import copy\n",
        "import math\n",
        "\n",
        "import numpy as np\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "import scipy\n",
        "import scipy.optimize\n",
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms.functional as tvtf\n",
        "\n",
        "from google.colab import drive"
      ],
      "metadata": {
        "id": "nJT0rEFvWZn7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Connecting Google Drive  \n",
        "\n",
        "In this part, I am connecting my Google Drive storage to Google Colab."
      ],
      "metadata": {
        "id": "MVnedT17wjmp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount('/content/drive')\n",
        "%cd /content/drive/MyDrive/CSE5CV_Assignment"
      ],
      "metadata": {
        "id": "rbXWe2qKWb66",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fe4ca890-7e0d-40e9-ea67-0cf57f862d64"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "/content/drive/MyDrive/CSE5CV_Assignment\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###  Loading and Previewing the First Frame of the Video  \n",
        "\n",
        "In this step, I check whether my video (`Task1.mp4`) is stored correctly in Google Drive and can be read inside Colab.  \n",
        "\n",
        "- I give the full path to my video file (`/content/drive/MyDrive/CSE5CV_Assignment/Task1.mp4`).  \n",
        "- `cv2.VideoCapture(filename)`: Opens the video file so we can read it frame by frame.  \n",
        "- `vid.read()`: Reads the very first frame of the video."
      ],
      "metadata": {
        "id": "EgqIBBRzwxcq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "filename = \"/content/drive/MyDrive/CSE5CV_Assignment/Task1.mp4\"\n",
        "\n",
        "# Try reading again\n",
        "vid = cv2.VideoCapture(filename)\n",
        "ok, img = vid.read()\n",
        "vid.release()\n",
        "\n",
        "if not ok or img is None:\n",
        "    raise FileNotFoundError(f\"Could not read first frame from {filename}\")\n",
        "\n",
        "# Convert and display\n",
        "img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.imshow(img)\n",
        "plt.axis(\"off\")\n",
        "plt.title(\"First frame preview\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "5Bwv_NYKWezi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###  Object Detection with Mask R-CNN  \n",
        "\n",
        "In this step, I run a **pre-trained deep learning model** (Mask R-CNN) on my video to detect objects in each frame.  \n",
        "\n",
        "- First, I define `preprocess_image()`, which converts an image into the format the model expects (a PyTorch tensor with a batch dimension).  \n",
        "- I load the **Mask R-CNN model with a ResNet-50 backbone**, which has already been trained on the COCO dataset (a large dataset with 80 everyday object categories).  \n",
        "- I set the model to **evaluation mode** (`eval()`) since I am not training it, only using it for predictions.  \n",
        "- If a GPU is available, I move the model to the GPU to make predictions faster.  \n",
        "- I reset the video to the first frame and calculate how many frames the video has in total.  \n",
        "- For each frame:\n",
        "  - I read the frame and convert it from BGR to RGB (since OpenCV and PyTorch use different color formats).  \n",
        "  - I pass the frame to Mask R-CNN, which returns three things:  \n",
        "    - `boxes`: the bounding box coordinates of detected objects.  \n",
        "    - `labels`: the predicted object categories.  \n",
        "    - `scores`: the confidence level of each detection.  \n",
        "  - I store these results for every frame in lists (`all_boxes`, `all_labels`, `all_scores`).  \n",
        "  - Every 20 frames, I print progress so I know how much is done.  \n",
        "- Finally, I save all the detections into `.pt` files so I don’t need to run the heavy detection process again later.  "
      ],
      "metadata": {
        "id": "vMPeECbLxChW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_image(image):\n",
        "    image = tvtf.to_tensor(image)\n",
        "    image = image.unsqueeze(dim=0)\n",
        "    return image\n",
        "\n",
        "maskrcnn = torchvision.models.detection.maskrcnn_resnet50_fpn(pretrained=True)\n",
        "maskrcnn.eval()\n",
        "if torch.cuda.is_available():\n",
        "    maskrcnn.cuda()\n",
        "\n",
        "# Go to the start of the video\n",
        "vid.set(cv2.CAP_PROP_POS_FRAMES, 0)\n",
        "\n",
        "# Record how long the video is (in frames)\n",
        "vid_length = int(vid.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "\n",
        "# For each frame, read it, give it to maskrcnn and record the detections\n",
        "all_boxes = []\n",
        "all_labels = []\n",
        "all_scores = []\n",
        "for i in range(vid_length):\n",
        "    _, img = vid.read()\n",
        "    if img is None:\n",
        "        break\n",
        "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        input_image = preprocess_image(img)\n",
        "        if torch.cuda.is_available():\n",
        "            input_image = input_image.cuda()\n",
        "        result = maskrcnn(input_image)[0]\n",
        "\n",
        "    all_boxes.append(result['boxes'].detach().cpu().numpy())\n",
        "    all_labels.append(result['labels'].detach().cpu().numpy())\n",
        "    all_scores.append(result['scores'].detach().cpu().numpy())\n",
        "    if i % 20 == 0:\n",
        "        print(f'{i+1:0d}/{vid_length}')\n",
        "\n",
        "torch.save(all_boxes, 'all_boxes.pt')\n",
        "torch.save(all_labels, 'all_labels.pt')\n",
        "torch.save(all_scores, 'all_scores.pt')"
      ],
      "metadata": {
        "id": "_3QC15GxWfZd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5f67e377-1f25-4583-a528-39b114cf159a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=MaskRCNN_ResNet50_FPN_Weights.COCO_V1`. You can also use `weights=MaskRCNN_ResNet50_FPN_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/maskrcnn_resnet50_fpn_coco-bf2d0c1e.pth\" to /root/.cache/torch/hub/checkpoints/maskrcnn_resnet50_fpn_coco-bf2d0c1e.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170M/170M [00:01<00:00, 158MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###  Reloading Saved Detections  \n",
        "\n",
        "In this step, I am **loading the results** that I previously saved when I ran the Mask R-CNN detection.  \n",
        "\n",
        "- `torch.load('all_boxes.pt')`, `torch.load('all_labels.pt')`, and `torch.load('all_scores.pt')` bring back the bounding box coordinates, object class labels, and confidence scores for every frame.  \n",
        "- This means I don’t need to re-run the heavy detection process again — I can just re-use the saved results.  \n",
        "- I also calculate how many frames were processed (`vid_length`) by checking the length of the loaded detections.  \n",
        "- Finally, I print a message confirming how many frames of the video now have detections ready.  \n"
      ],
      "metadata": {
        "id": "SEozYHEWxOmi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "all_boxes = torch.load('all_boxes.pt')\n",
        "all_labels = torch.load('all_labels.pt')\n",
        "all_scores = torch.load('all_scores.pt')\n",
        "\n",
        "vid_length = len(all_boxes)\n",
        "\n",
        "print(f'Loaded detections for {vid_length} video frames')"
      ],
      "metadata": {
        "id": "GQrkUgVGW4QO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "da68d1aa-e8ba-4cea-ab62-4fd0758d84d6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded detections for 0 video frames\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Task 2 — Filter & Visualise Detections"
      ],
      "metadata": {
        "id": "geKw6Pi-W9iZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this step, I am setting up the **Mask R-CNN model** that will detect objects in my video frames.  \n",
        "\n",
        "- Imported the main libraries:  \n",
        "  - `cv2` for working with video frames.  \n",
        "  - `torch` and `torchvision` for deep learning and pre-trained models.  \n",
        "  - `torchvision.transforms.functional` for image preprocessing.  \n",
        "  - `numpy` for handling arrays.  \n",
        "\n",
        "- Loaded **Mask R-CNN with a ResNet-50 backbone**. This model is already trained on the **COCO dataset** (80 everyday object categories like person, bottle, cup, laptop, etc.).  \n",
        "- Set the model to **evaluation mode** (`eval()`), which means it will only be used for predictions, not training.  \n",
        "- Chose whether to run the model on a **GPU** (if available) for speed, otherwise it defaults to the CPU.  \n"
      ],
      "metadata": {
        "id": "q-bGvojJxSGa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms.functional as T\n",
        "import numpy as np\n",
        "\n",
        "# Load pre-trained Mask R-CNN model with ResNet-50 backbone\n",
        "mask_rcnn_model = torchvision.models.detection.maskrcnn_resnet50_fpn(weights=\"DEFAULT\")\n",
        "mask_rcnn_model.eval()\n",
        "\n",
        "# Set device to GPU if available, otherwise CPU\n",
        "device_type = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "mask_rcnn_model.to(device_type)\n"
      ],
      "metadata": {
        "id": "UJNmo8AdW6TB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1577e83d-a9bc-40b2-cc3a-80837e8bb45e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MaskRCNN(\n",
              "  (transform): GeneralizedRCNNTransform(\n",
              "      Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
              "      Resize(min_size=(800,), max_size=1333, mode='bilinear')\n",
              "  )\n",
              "  (backbone): BackboneWithFPN(\n",
              "    (body): IntermediateLayerGetter(\n",
              "      (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
              "      (bn1): FrozenBatchNorm2d(64, eps=0.0)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
              "      (layer1): Sequential(\n",
              "        (0): Bottleneck(\n",
              "          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): FrozenBatchNorm2d(64, eps=0.0)\n",
              "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (bn2): FrozenBatchNorm2d(64, eps=0.0)\n",
              "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): FrozenBatchNorm2d(256, eps=0.0)\n",
              "          (relu): ReLU(inplace=True)\n",
              "          (downsample): Sequential(\n",
              "            (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "            (1): FrozenBatchNorm2d(256, eps=0.0)\n",
              "          )\n",
              "        )\n",
              "        (1): Bottleneck(\n",
              "          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): FrozenBatchNorm2d(64, eps=0.0)\n",
              "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (bn2): FrozenBatchNorm2d(64, eps=0.0)\n",
              "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): FrozenBatchNorm2d(256, eps=0.0)\n",
              "          (relu): ReLU(inplace=True)\n",
              "        )\n",
              "        (2): Bottleneck(\n",
              "          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): FrozenBatchNorm2d(64, eps=0.0)\n",
              "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (bn2): FrozenBatchNorm2d(64, eps=0.0)\n",
              "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): FrozenBatchNorm2d(256, eps=0.0)\n",
              "          (relu): ReLU(inplace=True)\n",
              "        )\n",
              "      )\n",
              "      (layer2): Sequential(\n",
              "        (0): Bottleneck(\n",
              "          (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): FrozenBatchNorm2d(128, eps=0.0)\n",
              "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "          (bn2): FrozenBatchNorm2d(128, eps=0.0)\n",
              "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): FrozenBatchNorm2d(512, eps=0.0)\n",
              "          (relu): ReLU(inplace=True)\n",
              "          (downsample): Sequential(\n",
              "            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "            (1): FrozenBatchNorm2d(512, eps=0.0)\n",
              "          )\n",
              "        )\n",
              "        (1): Bottleneck(\n",
              "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): FrozenBatchNorm2d(128, eps=0.0)\n",
              "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (bn2): FrozenBatchNorm2d(128, eps=0.0)\n",
              "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): FrozenBatchNorm2d(512, eps=0.0)\n",
              "          (relu): ReLU(inplace=True)\n",
              "        )\n",
              "        (2): Bottleneck(\n",
              "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): FrozenBatchNorm2d(128, eps=0.0)\n",
              "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (bn2): FrozenBatchNorm2d(128, eps=0.0)\n",
              "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): FrozenBatchNorm2d(512, eps=0.0)\n",
              "          (relu): ReLU(inplace=True)\n",
              "        )\n",
              "        (3): Bottleneck(\n",
              "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): FrozenBatchNorm2d(128, eps=0.0)\n",
              "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (bn2): FrozenBatchNorm2d(128, eps=0.0)\n",
              "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): FrozenBatchNorm2d(512, eps=0.0)\n",
              "          (relu): ReLU(inplace=True)\n",
              "        )\n",
              "      )\n",
              "      (layer3): Sequential(\n",
              "        (0): Bottleneck(\n",
              "          (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
              "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
              "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
              "          (relu): ReLU(inplace=True)\n",
              "          (downsample): Sequential(\n",
              "            (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "            (1): FrozenBatchNorm2d(1024, eps=0.0)\n",
              "          )\n",
              "        )\n",
              "        (1): Bottleneck(\n",
              "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
              "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
              "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
              "          (relu): ReLU(inplace=True)\n",
              "        )\n",
              "        (2): Bottleneck(\n",
              "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
              "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
              "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
              "          (relu): ReLU(inplace=True)\n",
              "        )\n",
              "        (3): Bottleneck(\n",
              "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
              "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
              "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
              "          (relu): ReLU(inplace=True)\n",
              "        )\n",
              "        (4): Bottleneck(\n",
              "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
              "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
              "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
              "          (relu): ReLU(inplace=True)\n",
              "        )\n",
              "        (5): Bottleneck(\n",
              "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
              "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
              "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
              "          (relu): ReLU(inplace=True)\n",
              "        )\n",
              "      )\n",
              "      (layer4): Sequential(\n",
              "        (0): Bottleneck(\n",
              "          (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): FrozenBatchNorm2d(512, eps=0.0)\n",
              "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "          (bn2): FrozenBatchNorm2d(512, eps=0.0)\n",
              "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): FrozenBatchNorm2d(2048, eps=0.0)\n",
              "          (relu): ReLU(inplace=True)\n",
              "          (downsample): Sequential(\n",
              "            (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "            (1): FrozenBatchNorm2d(2048, eps=0.0)\n",
              "          )\n",
              "        )\n",
              "        (1): Bottleneck(\n",
              "          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): FrozenBatchNorm2d(512, eps=0.0)\n",
              "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (bn2): FrozenBatchNorm2d(512, eps=0.0)\n",
              "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): FrozenBatchNorm2d(2048, eps=0.0)\n",
              "          (relu): ReLU(inplace=True)\n",
              "        )\n",
              "        (2): Bottleneck(\n",
              "          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): FrozenBatchNorm2d(512, eps=0.0)\n",
              "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (bn2): FrozenBatchNorm2d(512, eps=0.0)\n",
              "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): FrozenBatchNorm2d(2048, eps=0.0)\n",
              "          (relu): ReLU(inplace=True)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (fpn): FeaturePyramidNetwork(\n",
              "      (inner_blocks): ModuleList(\n",
              "        (0): Conv2dNormActivation(\n",
              "          (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
              "        )\n",
              "        (1): Conv2dNormActivation(\n",
              "          (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
              "        )\n",
              "        (2): Conv2dNormActivation(\n",
              "          (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
              "        )\n",
              "        (3): Conv2dNormActivation(\n",
              "          (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n",
              "        )\n",
              "      )\n",
              "      (layer_blocks): ModuleList(\n",
              "        (0-3): 4 x Conv2dNormActivation(\n",
              "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        )\n",
              "      )\n",
              "      (extra_blocks): LastLevelMaxPool()\n",
              "    )\n",
              "  )\n",
              "  (rpn): RegionProposalNetwork(\n",
              "    (anchor_generator): AnchorGenerator()\n",
              "    (head): RPNHead(\n",
              "      (conv): Sequential(\n",
              "        (0): Conv2dNormActivation(\n",
              "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "          (1): ReLU(inplace=True)\n",
              "        )\n",
              "      )\n",
              "      (cls_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))\n",
              "      (bbox_pred): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))\n",
              "    )\n",
              "  )\n",
              "  (roi_heads): RoIHeads(\n",
              "    (box_roi_pool): MultiScaleRoIAlign(featmap_names=['0', '1', '2', '3'], output_size=(7, 7), sampling_ratio=2)\n",
              "    (box_head): TwoMLPHead(\n",
              "      (fc6): Linear(in_features=12544, out_features=1024, bias=True)\n",
              "      (fc7): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "    )\n",
              "    (box_predictor): FastRCNNPredictor(\n",
              "      (cls_score): Linear(in_features=1024, out_features=91, bias=True)\n",
              "      (bbox_pred): Linear(in_features=1024, out_features=364, bias=True)\n",
              "    )\n",
              "    (mask_roi_pool): MultiScaleRoIAlign(featmap_names=['0', '1', '2', '3'], output_size=(14, 14), sampling_ratio=2)\n",
              "    (mask_head): MaskRCNNHeads(\n",
              "      (0): Conv2dNormActivation(\n",
              "        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        (1): ReLU(inplace=True)\n",
              "      )\n",
              "      (1): Conv2dNormActivation(\n",
              "        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        (1): ReLU(inplace=True)\n",
              "      )\n",
              "      (2): Conv2dNormActivation(\n",
              "        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        (1): ReLU(inplace=True)\n",
              "      )\n",
              "      (3): Conv2dNormActivation(\n",
              "        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        (1): ReLU(inplace=True)\n",
              "      )\n",
              "    )\n",
              "    (mask_predictor): MaskRCNNPredictor(\n",
              "      (conv5_mask): ConvTranspose2d(256, 256, kernel_size=(2, 2), stride=(2, 2))\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (mask_fcn_logits): Conv2d(256, 91, kernel_size=(1, 1), stride=(1, 1))\n",
              "    )\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###  Setting Up the Video and Target Classes  \n",
        "\n",
        "In this part, I prepare my video file and decide **which objects I want the model to detect**.  \n",
        "\n",
        "- First, I set the path to my video (`Task1.mp4`) and open it with OpenCV’s `VideoCapture`.  \n",
        "- Next, I define the **COCO dataset classes** that are relevant for my assignment:  \n",
        "  - `1: person`  \n",
        "  - `44: bottle`  \n",
        "  - `47: cup`  \n",
        "  - `63: laptop`  \n",
        "  - `65: remote`  \n",
        "- I then put these into a set (`target_classes`) so the model only pays attention to these categories instead of all 80 classes in COCO.  \n",
        "- Finally, I define **confidence thresholds** for each class. This sets the minimum probability required for a detection to count:  \n",
        "  - For example, a detection of a \"person\" must have at least 80% confidence, but for \"remote\" (which is smaller and harder to detect), I allow a lower threshold of 30%.  \n"
      ],
      "metadata": {
        "id": "8A-_YPzmxcs5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Video file path\n",
        "video_file = 'Task1.mp4'\n",
        "video_capture = cv2.VideoCapture(video_file)\n",
        "\n",
        "# Define COCO dataset classes and the ones we are interested in\n",
        "coco_classes = {\n",
        "    1:  \"person\",\n",
        "    44: \"bottle\",\n",
        "    47: \"cup\",\n",
        "    63: \"laptop\",\n",
        "    65: \"remote\"\n",
        "}\n",
        "target_classes = {1, 44, 47, 63, 65}\n",
        "\n",
        "# Define detection confidence thresholds per class\n",
        "confidence_thresholds = {1: 0.8, 44: 0.7, 47: 0.6, 63: 0.6, 65: 0.3}\n"
      ],
      "metadata": {
        "id": "w9fc1bR2XBl4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Preprocessing Video Frames  \n",
        "\n",
        "Before sending a video frame to the detection model, I need to **convert it into the right format**.  \n",
        "\n",
        "- The function `preprocess_frame(frame)` takes a single frame from the video.  \n",
        "- If the frame is empty (for example, if the video ended or could not be read), it raises an error so I know something went wrong.  \n",
        "- `cv2.cvtColor` changes the image colors from **BGR (default in OpenCV)** to **RGB** (which most deep learning models use).  \n",
        "- `T.to_tensor` converts the image into a PyTorch tensor (the data format required for the model).  \n",
        "- `.unsqueeze(0)` adds a new dimension so that the frame looks like a batch of images (even though it’s just one image).  \n",
        "- Finally, `.to(device_type)` moves the tensor to either **GPU or CPU** depending on what is available.  \n"
      ],
      "metadata": {
        "id": "3lYUEPxVxmPY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to preprocess video frames for model input\n",
        "def preprocess_frame(frame):\n",
        "    if frame is None:\n",
        "        raise ValueError(\"Error: Captured frame is empty.\")\n",
        "    rgb_image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "    tensor_image = T.to_tensor(rgb_image).unsqueeze(0).to(device_type)\n",
        "    return tensor_image\n"
      ],
      "metadata": {
        "id": "Y5E8qbHbXDkx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ▶️ Running Detection on Video Frames  \n",
        "\n",
        "In this section, I actually run the object detection model on my video, frame by frame.  \n"
      ],
      "metadata": {
        "id": "rVFU6LWXxsRs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize frame counter\n",
        "frame_counter = 0\n",
        "\n",
        "# Process the video frame-by-frame\n",
        "while video_capture.isOpened():\n",
        "    ret, frame = video_capture.read()\n",
        "    if not ret or frame is None:\n",
        "        print(\"End of video or failed to read frame.\")\n",
        "        break\n",
        "\n",
        "    frame_counter += 1\n",
        "\n",
        "    # Process only every 10th frame\n",
        "    if frame_counter % 10 != 0:\n",
        "        continue\n",
        "\n",
        "    # Preprocess the frame and run the model\n",
        "    input_tensor = preprocess_frame(frame)\n",
        "    with torch.no_grad():\n",
        "        outputs = mask_rcnn_model(input_tensor)[0]\n",
        "\n",
        "    # Draw bounding boxes on the frame for selected detections\n",
        "    for i in range(len(outputs['labels'])):\n",
        "        class_id = outputs['labels'][i].item()\n",
        "        score = outputs['scores'][i].item()\n",
        "\n",
        "        # Filter based on selected classes and confidence thresholds\n",
        "        if class_id in target_classes and score >= confidence_thresholds[class_id]:\n",
        "            bbox = outputs['boxes'][i].cpu().numpy().astype(int)\n",
        "            label = coco_classes[class_id]\n",
        "\n",
        "            # Draw the bounding box and label on the frame\n",
        "            cv2.rectangle(frame, (bbox[0], bbox[1]), (bbox[2], bbox[3]), (0, 255, 0), 2)\n",
        "            text = f\"{label}: {score:.2f}\"\n",
        "            cv2.putText(frame, text, (bbox[0], bbox[1] - 10),\n",
        "                        cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
        "\n",
        "    # Display the frame with detections\n",
        "    from google.colab.patches import cv2_imshow  # Colab-specific display function\n",
        "    cv2_imshow(frame)\n"
      ],
      "metadata": {
        "id": "TGAuONmUOGIS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Release video resources\n",
        "video_capture.release()\n",
        "print(\"Detection process completed.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RpBmUU2hOH-z",
        "outputId": "45c36ab8-a418-485f-a465-1760e11f85d7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Detection process completed.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Task 3\n"
      ],
      "metadata": {
        "id": "CWcpoqNrx-S6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms.functional as T\n",
        "import numpy as np\n",
        "from collections import defaultdict\n",
        "from scipy.spatial import distance\n",
        "\n",
        "# Load pre-trained Mask R-CNN model with ResNet-50 backbone\n",
        "tracking_model = torchvision.models.detection.maskrcnn_resnet50_fpn(weights=\"DEFAULT\")\n",
        "tracking_model.eval()\n",
        "\n",
        "# Set device to GPU if available, otherwise use CPU\n",
        "active_device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "tracking_model.to(active_device)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uI_f4rCkOKmj",
        "outputId": "c03f2b3f-28f3-4657-f1cc-e121ee04ea5b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MaskRCNN(\n",
              "  (transform): GeneralizedRCNNTransform(\n",
              "      Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
              "      Resize(min_size=(800,), max_size=1333, mode='bilinear')\n",
              "  )\n",
              "  (backbone): BackboneWithFPN(\n",
              "    (body): IntermediateLayerGetter(\n",
              "      (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
              "      (bn1): FrozenBatchNorm2d(64, eps=0.0)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
              "      (layer1): Sequential(\n",
              "        (0): Bottleneck(\n",
              "          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): FrozenBatchNorm2d(64, eps=0.0)\n",
              "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (bn2): FrozenBatchNorm2d(64, eps=0.0)\n",
              "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): FrozenBatchNorm2d(256, eps=0.0)\n",
              "          (relu): ReLU(inplace=True)\n",
              "          (downsample): Sequential(\n",
              "            (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "            (1): FrozenBatchNorm2d(256, eps=0.0)\n",
              "          )\n",
              "        )\n",
              "        (1): Bottleneck(\n",
              "          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): FrozenBatchNorm2d(64, eps=0.0)\n",
              "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (bn2): FrozenBatchNorm2d(64, eps=0.0)\n",
              "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): FrozenBatchNorm2d(256, eps=0.0)\n",
              "          (relu): ReLU(inplace=True)\n",
              "        )\n",
              "        (2): Bottleneck(\n",
              "          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): FrozenBatchNorm2d(64, eps=0.0)\n",
              "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (bn2): FrozenBatchNorm2d(64, eps=0.0)\n",
              "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): FrozenBatchNorm2d(256, eps=0.0)\n",
              "          (relu): ReLU(inplace=True)\n",
              "        )\n",
              "      )\n",
              "      (layer2): Sequential(\n",
              "        (0): Bottleneck(\n",
              "          (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): FrozenBatchNorm2d(128, eps=0.0)\n",
              "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "          (bn2): FrozenBatchNorm2d(128, eps=0.0)\n",
              "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): FrozenBatchNorm2d(512, eps=0.0)\n",
              "          (relu): ReLU(inplace=True)\n",
              "          (downsample): Sequential(\n",
              "            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "            (1): FrozenBatchNorm2d(512, eps=0.0)\n",
              "          )\n",
              "        )\n",
              "        (1): Bottleneck(\n",
              "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): FrozenBatchNorm2d(128, eps=0.0)\n",
              "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (bn2): FrozenBatchNorm2d(128, eps=0.0)\n",
              "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): FrozenBatchNorm2d(512, eps=0.0)\n",
              "          (relu): ReLU(inplace=True)\n",
              "        )\n",
              "        (2): Bottleneck(\n",
              "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): FrozenBatchNorm2d(128, eps=0.0)\n",
              "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (bn2): FrozenBatchNorm2d(128, eps=0.0)\n",
              "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): FrozenBatchNorm2d(512, eps=0.0)\n",
              "          (relu): ReLU(inplace=True)\n",
              "        )\n",
              "        (3): Bottleneck(\n",
              "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): FrozenBatchNorm2d(128, eps=0.0)\n",
              "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (bn2): FrozenBatchNorm2d(128, eps=0.0)\n",
              "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): FrozenBatchNorm2d(512, eps=0.0)\n",
              "          (relu): ReLU(inplace=True)\n",
              "        )\n",
              "      )\n",
              "      (layer3): Sequential(\n",
              "        (0): Bottleneck(\n",
              "          (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
              "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
              "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
              "          (relu): ReLU(inplace=True)\n",
              "          (downsample): Sequential(\n",
              "            (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "            (1): FrozenBatchNorm2d(1024, eps=0.0)\n",
              "          )\n",
              "        )\n",
              "        (1): Bottleneck(\n",
              "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
              "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
              "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
              "          (relu): ReLU(inplace=True)\n",
              "        )\n",
              "        (2): Bottleneck(\n",
              "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
              "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
              "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
              "          (relu): ReLU(inplace=True)\n",
              "        )\n",
              "        (3): Bottleneck(\n",
              "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
              "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
              "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
              "          (relu): ReLU(inplace=True)\n",
              "        )\n",
              "        (4): Bottleneck(\n",
              "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
              "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
              "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
              "          (relu): ReLU(inplace=True)\n",
              "        )\n",
              "        (5): Bottleneck(\n",
              "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
              "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
              "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
              "          (relu): ReLU(inplace=True)\n",
              "        )\n",
              "      )\n",
              "      (layer4): Sequential(\n",
              "        (0): Bottleneck(\n",
              "          (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): FrozenBatchNorm2d(512, eps=0.0)\n",
              "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "          (bn2): FrozenBatchNorm2d(512, eps=0.0)\n",
              "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): FrozenBatchNorm2d(2048, eps=0.0)\n",
              "          (relu): ReLU(inplace=True)\n",
              "          (downsample): Sequential(\n",
              "            (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "            (1): FrozenBatchNorm2d(2048, eps=0.0)\n",
              "          )\n",
              "        )\n",
              "        (1): Bottleneck(\n",
              "          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): FrozenBatchNorm2d(512, eps=0.0)\n",
              "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (bn2): FrozenBatchNorm2d(512, eps=0.0)\n",
              "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): FrozenBatchNorm2d(2048, eps=0.0)\n",
              "          (relu): ReLU(inplace=True)\n",
              "        )\n",
              "        (2): Bottleneck(\n",
              "          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): FrozenBatchNorm2d(512, eps=0.0)\n",
              "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (bn2): FrozenBatchNorm2d(512, eps=0.0)\n",
              "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): FrozenBatchNorm2d(2048, eps=0.0)\n",
              "          (relu): ReLU(inplace=True)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (fpn): FeaturePyramidNetwork(\n",
              "      (inner_blocks): ModuleList(\n",
              "        (0): Conv2dNormActivation(\n",
              "          (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
              "        )\n",
              "        (1): Conv2dNormActivation(\n",
              "          (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
              "        )\n",
              "        (2): Conv2dNormActivation(\n",
              "          (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
              "        )\n",
              "        (3): Conv2dNormActivation(\n",
              "          (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n",
              "        )\n",
              "      )\n",
              "      (layer_blocks): ModuleList(\n",
              "        (0-3): 4 x Conv2dNormActivation(\n",
              "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        )\n",
              "      )\n",
              "      (extra_blocks): LastLevelMaxPool()\n",
              "    )\n",
              "  )\n",
              "  (rpn): RegionProposalNetwork(\n",
              "    (anchor_generator): AnchorGenerator()\n",
              "    (head): RPNHead(\n",
              "      (conv): Sequential(\n",
              "        (0): Conv2dNormActivation(\n",
              "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "          (1): ReLU(inplace=True)\n",
              "        )\n",
              "      )\n",
              "      (cls_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))\n",
              "      (bbox_pred): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))\n",
              "    )\n",
              "  )\n",
              "  (roi_heads): RoIHeads(\n",
              "    (box_roi_pool): MultiScaleRoIAlign(featmap_names=['0', '1', '2', '3'], output_size=(7, 7), sampling_ratio=2)\n",
              "    (box_head): TwoMLPHead(\n",
              "      (fc6): Linear(in_features=12544, out_features=1024, bias=True)\n",
              "      (fc7): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "    )\n",
              "    (box_predictor): FastRCNNPredictor(\n",
              "      (cls_score): Linear(in_features=1024, out_features=91, bias=True)\n",
              "      (bbox_pred): Linear(in_features=1024, out_features=364, bias=True)\n",
              "    )\n",
              "    (mask_roi_pool): MultiScaleRoIAlign(featmap_names=['0', '1', '2', '3'], output_size=(14, 14), sampling_ratio=2)\n",
              "    (mask_head): MaskRCNNHeads(\n",
              "      (0): Conv2dNormActivation(\n",
              "        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        (1): ReLU(inplace=True)\n",
              "      )\n",
              "      (1): Conv2dNormActivation(\n",
              "        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        (1): ReLU(inplace=True)\n",
              "      )\n",
              "      (2): Conv2dNormActivation(\n",
              "        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        (1): ReLU(inplace=True)\n",
              "      )\n",
              "      (3): Conv2dNormActivation(\n",
              "        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        (1): ReLU(inplace=True)\n",
              "      )\n",
              "    )\n",
              "    (mask_predictor): MaskRCNNPredictor(\n",
              "      (conv5_mask): ConvTranspose2d(256, 256, kernel_size=(2, 2), stride=(2, 2))\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (mask_fcn_logits): Conv2d(256, 91, kernel_size=(1, 1), stride=(1, 1))\n",
              "    )\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###  Setting Up Video Output and Tracking Structures  \n",
        "\n",
        "In this step, I prepare everything needed to save the **tracked video** with detections drawn on it.  \n",
        "\n",
        "- I specify the **input video** (`Task1.mp4`) and the name of the **output video** that will be created with tracking results (`Task3.mp4`).  \n",
        "- Using OpenCV, I read the video dimensions (width and height) so that the output video has the same size.  \n",
        "- I also set up a **video writer** with the XVID codec so the new video can be saved frame by frame at 30 fps.  \n",
        "- I define the **object classes** (from the COCO dataset) that I want to track: person, bottle, cup, laptop, and remote.  \n",
        "- I assign **confidence thresholds** for each class so that only detections with enough confidence are accepted.  \n",
        "- Finally, I prepare data structures (`object_trackers` and `track_ids`) that will be used to store and assign unique IDs to objects across frames.  \n"
      ],
      "metadata": {
        "id": "Yn9Gd92iyOMJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define paths for input and output video\n",
        "source_video = 'Task1.mp4'\n",
        "output_tracked_video = 'Task3.mp4'\n",
        "video_stream = cv2.VideoCapture(source_video)\n",
        "\n",
        "# Get frame width and height\n",
        "frame_width = int(video_stream.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "frame_height = int(video_stream.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "\n",
        "# Set up the video writer with the proper codec\n",
        "video_codec = cv2.VideoWriter_fourcc(*'XVID')  # Changed from 'mp4v' to 'XVID'\n",
        "output_writer = cv2.VideoWriter(output_tracked_video, video_codec, 30.0, (frame_width, frame_height))\n",
        "\n",
        "# COCO class mapping and target classes\n",
        "class_map = {1: 'person', 44: 'bottle', 47: 'cup', 63: 'laptop', 65: 'remote'}\n",
        "targeted_classes = {1, 44, 47, 63, 65}\n",
        "\n",
        "# Confidence thresholds for each class\n",
        "class_conf_thresholds = {1: 0.8, 44: 0.7, 47: 0.6, 63: 0.6, 65: 0.3}\n",
        "\n",
        "# Initialize tracking-related structures\n",
        "object_trackers = defaultdict(dict)\n",
        "track_ids = defaultdict(int)\n"
      ],
      "metadata": {
        "id": "a8wfTzckP_Va"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###  Converting a Frame into Model Input  \n",
        "\n",
        "This function prepares each video frame so that it can be understood by the deep learning model.  \n"
      ],
      "metadata": {
        "id": "fly2KBJ7yXvk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to preprocess video frames for model input\n",
        "def frame_to_tensor(frame_data):\n",
        "    if frame_data is None:\n",
        "        raise ValueError(\"Captured an empty frame.\")\n",
        "    rgb_image = cv2.cvtColor(frame_data, cv2.COLOR_BGR2RGB)\n",
        "    tensor_image = T.to_tensor(rgb_image).unsqueeze(0).to(active_device)\n",
        "    return tensor_image\n"
      ],
      "metadata": {
        "id": "0iAOD-8uQB4K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tracking Objects Across Frames  \n",
        "\n",
        "This section defines the helper functions that allow me to **track objects from one frame to the next**.  \n",
        "\n",
        "1. **`compute_box_center(box_coords)`**  \n",
        "   - Takes the coordinates of a bounding box (`x1, y1, x2, y2`).  \n",
        "   - Returns the center point of that box as `(x, y)`.  \n",
        "   - The center is useful for checking how far an object has moved between frames.  \n",
        "\n",
        "2. **`track_objects(detections, cls_id)`**  \n",
        "   - Updates the list of tracked objects for a given class (e.g., person, bottle, laptop).  \n",
        "   - For every detection in the current frame:  \n",
        "     - Computes its center.  \n",
        "     - Looks at existing tracked objects of the same class.  \n",
        "     - Finds the closest one by measuring the **Euclidean distance** between centers.  "
      ],
      "metadata": {
        "id": "nVheZudD1sP5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to compute the center of the bounding box\n",
        "def compute_box_center(box_coords):\n",
        "    x1, y1, x2, y2 = box_coords\n",
        "    return (int((x1 + x2) / 2), int((y1 + y2) / 2))\n",
        "\n",
        "# Function to update trackers based on distance between box centers\n",
        "def track_objects(detections, cls_id):\n",
        "    global track_ids, object_trackers\n",
        "\n",
        "    updated_tracker_data = {}  # Temporary dictionary to hold updated tracks\n",
        "\n",
        "    # Loop through each detection for the current class\n",
        "    for detection_data in detections:\n",
        "        bbox = detection_data['bbox']\n",
        "        score = detection_data['score']\n",
        "        center = compute_box_center(bbox)\n",
        "\n",
        "        # Find the closest existing tracker for the same class\n",
        "        min_distance = float('inf')\n",
        "        closest_tracker = None\n",
        "\n",
        "        for t_id, tracking_data in object_trackers[cls_id].items():\n",
        "            tracked_center = tracking_data['center']\n",
        "            dist = distance.euclidean(center, tracked_center)\n",
        "            if dist < min_distance:\n",
        "                min_distance = dist\n",
        "                closest_tracker = t_id\n",
        "\n",
        "        # If a close match is found, update the tracker\n",
        "        if min_distance < 50:  # Adjust this threshold for better accuracy\n",
        "            updated_tracker_data[closest_tracker] = {'center': center, 'bbox': bbox, 'score': score}\n",
        "        else:\n",
        "            # Create a new track if no suitable match is found\n",
        "            new_tracker_id = track_ids[cls_id]\n",
        "            track_ids[cls_id] += 1\n",
        "            updated_tracker_data[new_tracker_id] = {'center': center, 'bbox': bbox, 'score': score}\n",
        "\n",
        "    # Update the global trackers for the current class\n",
        "    object_trackers[cls_id] = updated_tracker_data\n"
      ],
      "metadata": {
        "id": "aWup-kiXQDYL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###  Frame-by-Frame Tracking and Drawing Results  \n",
        "\n",
        "This is the **main loop** where the video is processed one frame at a time, detections are made, and objects are tracked across frames.  \n",
        "\n",
        "- The loop runs while the video is open. If no frame can be read, it stops.  \n",
        "- Each frame is preprocessed (`frame_to_tensor`) and passed into the detection model to get predictions.  \n",
        "- For every prediction, I collect:  \n",
        "  - the **class ID** (e.g., person, bottle, laptop),  \n",
        "  - the **confidence score**, and  \n",
        "  - the **bounding box coordinates**.  \n",
        "- I only keep detections that belong to my **target classes** and are above their confidence thresholds.  \n",
        "- For each class, I update the **object trackers** using the `track_objects()` function. This ensures that the same object keeps its ID across frames.  \n",
        "- Then, for every tracked object, I draw:  \n",
        "  - a **green bounding box** around it,  \n",
        "  - a **red dot** at its center,  \n",
        "  - and a label showing the object’s name, its unique ID, and the confidence score.  \n",
        "- Finally, the processed frame (with boxes, IDs, and labels) is saved into the **output video file** using `output_writer.write()`.  \n"
      ],
      "metadata": {
        "id": "T5bqNJ_y18j9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Process the video frame-by-frame\n",
        "while video_stream.isOpened():\n",
        "    ret, current_frame = video_stream.read()\n",
        "    if not ret or current_frame is None:\n",
        "        print(\"End of video reached or failed to capture frame.\")\n",
        "        break\n",
        "\n",
        "    # Preprocess the frame and perform object detection\n",
        "    frame_tensor = frame_to_tensor(current_frame)\n",
        "    with torch.no_grad():\n",
        "        predictions = tracking_model(frame_tensor)[0]\n",
        "\n",
        "    # Gather detections by class\n",
        "    class_detections = defaultdict(list)\n",
        "\n",
        "    for i in range(len(predictions['labels'])):\n",
        "        cls_id = predictions['labels'][i].item()\n",
        "        confidence = predictions['scores'][i].item()\n",
        "        bbox = predictions['boxes'][i].cpu().numpy().astype(int)\n",
        "\n",
        "        # Filter by selected classes and confidence thresholds\n",
        "        if cls_id in targeted_classes and confidence >= class_conf_thresholds[cls_id]:\n",
        "            class_detections[cls_id].append({'bbox': bbox, 'score': confidence})\n",
        "\n",
        "    # Update object trackers for each class\n",
        "    for cls_id, detections in class_detections.items():\n",
        "        track_objects(detections, cls_id)\n",
        "\n",
        "    # Draw bounding boxes and track information on the frame\n",
        "    for cls_id, tracked_objects in object_trackers.items():\n",
        "        for tracker_id, track_data in tracked_objects.items():\n",
        "            bbox = track_data['bbox']\n",
        "            center = track_data['center']\n",
        "            score = track_data['score']\n",
        "            label = class_map[cls_id]\n",
        "\n",
        "            # Draw bounding box, center point, and track ID on the frame\n",
        "            cv2.rectangle(current_frame, (bbox[0], bbox[1]), (bbox[2], bbox[3]), (0, 255, 0), 2)\n",
        "            cv2.circle(current_frame, center, 3, (0, 0, 255), -1)\n",
        "            track_text = f\"{label} ID: {tracker_id} | {score:.2f}\"\n",
        "            cv2.putText(current_frame, track_text, (bbox[0], bbox[1] - 10),\n",
        "                        cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 0, 0), 2)\n",
        "\n",
        "    # Write the processed frame to the output video\n",
        "    output_writer.write(current_frame)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2CVLuVf3QFf4",
        "outputId": "6ecfc204-05ee-49c1-ec50-7b5c7a9ed2a4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "End of video reached or failed to capture frame.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Release video resources\n",
        "video_stream.release()\n",
        "output_writer.release()  # Ensure this is called to properly close the video file\n",
        "\n",
        "\n",
        "print(\"Object tracking completed. The result is saved as Task3.mp4.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J2yQ41Z-QHXR",
        "outputId": "7078e5e7-0e89-4aee-af47-35e3849882ed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Object tracking completed. The result is saved as Task3.mp4.\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}